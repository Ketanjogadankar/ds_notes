{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLM (Masked Language Modelling)\n",
    "\n",
    "In order to achieve bidirectional representation , 15% of tokens in the input sentence are masked at random.<br>\n",
    "It uses AE(Autoencoder) approach. The AE Language Model aims to reconstruct the original data from corrupted inputs.<br>\n",
    "In BERT, pre-training input data is corrupted by adding [MASK]. For example, ‘Goa has the most beautiful beaches in India’ will become ‘Goa has the most beautiful [MASK]in India’ and objective of the model will be to predict the [MASK] word based on the context words. The advantage of Autoencoder language model is, that it can see the context on both forward and backward direction. However, due to the addition of [MASK] in the input data introduces a discrepancy in fine-tuning the model.<br>\n",
    "\n",
    "### Disadvantages of MLM:\n",
    "1. The discrepancy in fine-tuning due to masking <br>\n",
    "BERT is trained to predict tokens replaced with the special[MASK]token. The problem is that the [MASK] tokens never appear while fine-tuning BERT on downstream tasks. \n",
    "\t\n",
    "2. Predicted tokens are independent of each other <br>\n",
    "BERT assumes the predicted (masked) tokens are independent of each other given the unmasked tokens. To understand this let’s go through one example. <br>\n",
    "Whenever she goes to the [MASK] [MASK] she buys a lot of [MASK]. <br>\n",
    "This can be filled as <br>\n",
    "*Whenever she goes to the shopping centre, she buys a lot of clothes.*<br>\n",
    "Or<br>\n",
    "*Whenever she goes to the cinema hall she buys a lot of popcorn.*<br>\n",
    "but, the sentence<br>\n",
    "*Whenever she goes to the cinema hall she buys a lot of clothes.*<br>\n",
    "is not valid. BERT predicts all masked positions in parallel, meaning that during training, it does not learn to handle dependencies between predicting simultaneously masked tokens.\n",
    "\t\n",
    "**BERT is an example of MLM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLM (Permuted Language Modelling)\n",
    "\n",
    "PLM is an autoregressive model that aims to address/solve the limitations presented by BERT and LSTMs, and it outperforms BERT on a number of tasks.<br>\n",
    "Given a sequence x, an auto-regressive model (like an LSTM or XLNet) is a model that calculates the probability that $P(x_i | x<i)$ (the | stands for given, so this essentially means that the model predicts the probability of the ith word occurring in the sequence given the probability of all 0…i-1 words occurring in the sequence). These kinds of models (like LSTM’s) are asymmetric and aren’t learning from all the token relations in the corpus. LSTMs are only autoregressive models over 1 possible permutation of the sequence: the order of the tokens from left to right.<br>\n",
    "Other types of auto-regressive models such as ELMo allow a model to learn contextual representations of a token. In the specific case of ELMo, it could allow a model to learn P(xi | x>i) or contextual information in reversed order. Similar to LSTM’s, A model like ELMo is autoregressive over 2 possible permutations of the sequence, forwards and backwards. Beyond just ELMo, there are many other possible combinations to look at where the model could learn some interesting relations between tokens. For example, P(xi | xi-1, xi+1), could allow for some interesting embeddings. Essentially, any combination of tokens could allow an autoregressive model to learn interesting relations, and that is the fundamental basis behind permutative language modeling.<br>\n",
    "\n",
    "XLNet essentially does what I described above but for all the different permutations of a sequence. Consider a sequence [Dogs, are, all, mammals] with N=4 tokens. Look at the set of all 4! permutations $Z = [[1,2,3,4], [1,3,2,4] … [4,3,2,1]]$. XLNet is a model that is autoregressive over all such permutations. It predicts the probability of a token xi, given all the previous tokens $x<i$ and it can calculate the probability for any permutation of the sequence.<br>\n",
    "\n",
    "For example, it can calculate the probability of the 3rd element given the preceding elements and it does this for any permutation so that it can see which combination of preceding elements would most likely (highest probability) result in the correct 3rd element, which is the word all. Considering all 4! permutations, XLNet takes into consideration all the possible dependencies between tokens.<br>\n",
    "\n",
    "**Disadvantages of PLM:**<br>\n",
    "1. While permutative language modeling is the primary contribution of the paper, and it did succeed in overcoming the masked language modeling problem, it has some drawbacks. Firstly — and most obviously — XLNet is generally more computationally expensive and taksed longer to train as compared to BERT.<br>\n",
    "2. Another weakness of XLNet is that it can underperform on smaller/shorter input sentences. In XLNet’s pretraining, it is primarily pretrained on long input sequences and permutative language modeling is designed to capture long-term dependencies, so XLNet is not good on short inputs.<br>\n",
    "3. XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
