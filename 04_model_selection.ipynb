{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection is the problem of choosing one from among a set of candidate models.<br>\n",
    "It is common to choose a model that performs the best on a hold-out test dataset or to estimate model performance using a resampling technique, such as k-fold cross-validation.<br>\n",
    "**<mark>An alternative approach to model selection involves using probabilistic statistical measures that attempt to quantify both the model performance on the training dataset and the complexity of the model. Examples include the Akaike and Bayesian Information Criterion and the Minimum Description Length.<br>\n",
    "The benefit of these information criterion statistics is that they do not require a hold-out test set, although a limitation is that they do not take the uncertainty of the models into account and may end-up selecting models that are too simple.<br></mark>**\n",
    "In this post, you will discover probabilistic statistics for machine learning model selection.\n",
    "\n",
    "After reading this post, you will know:<br>\n",
    "- Model selection is the challenge of choosing one among a set of candidate models.\n",
    "- Akaike and Bayesian Information Criterion are two ways of scoring a model based on its log-likelihood and complexity.\n",
    "- Minimum Description Length provides another scoring method from information theory that can be shown to be equivalent to BIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge of Model Selection\n",
    "Model selection is the process of fitting multiple models on a given dataset and choosing one over all others.<br>\n",
    "Model selection: estimating the performance of different models in order to choose the best one.<br>\n",
    "This may apply in unsupervised learning, e.g. choosing a clustering model, or supervised learning, e.g. choosing a predictive model for a regression or classification task. It may also be a sub-task of modelling, such as feature selection for a given model.<br>\n",
    "\n",
    "There are many common approaches that may be used for model selection. For example, in the case of supervised learning, the three most common approaches are:\n",
    "- Train, Validation, and Test datasets.\n",
    "- Resampling Methods.\n",
    "- Probabilistic Statistics.\n",
    "\n",
    "The simplest reliable method of model selection involves fitting candidate models on a training set, tuning them on the validation dataset, and selecting a model that performs the best on the test dataset according to a chosen metric, such as accuracy or error. A problem with this approach is that it requires a lot of data.<br>\n",
    "\n",
    "Resampling techniques attempt to achieve the same as the train/val/test approach to model selection, although using a small dataset. An example is k-fold cross-validation where a training set is split into many train/test pairs and a model is fit and evaluated on each. This is repeated for each model and a model is selected with the best average score across the k-folds. A problem with this and the prior approach is that only model performance is assessed, regardless of model complexity.<br>\n",
    "\n",
    "**A third approach to model selection attempts to combine the complexity of the model with the performance of the model into a score, then select the model that minimizes or maximizes the score.**\n",
    "We can refer to this approach as statistical or probabilistic model selection as the scoring method uses a probabilistic framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Model Selection\n",
    "\n",
    "Probabilistic model selection (or “information criteria”) provides an analytical technique for scoring and choosing among candidate models.<br>\n",
    "<mark>Models are scored both on their performance on the training dataset and based on the complexity of the model.</mark><br>\n",
    "- **Model Performance.**: How well a candidate model has performed on the training dataset.\n",
    "- **Model Complexity.**: How complicated the trained candidate model is after training.\n",
    "Model performance may be evaluated using a probabilistic framework, such as log-likelihood under the framework of maximum likelihood estimation. Model complexity may be evaluated as the number of degrees of freedom or parameters in the model.<br>\n",
    "Historically various ‘information criteria’ have been proposed that attempt to correct for the bias of maximum likelihood by the addition of a penalty term to compensate for the over-fitting of more complex models.<br>\n",
    "\n",
    "**<mark>A benefit of probabilistic model selection methods is that a test dataset is not required, meaning that all of the data can be used to fit the model, and the final model that will be used for prediction in the domain can be scored directly.</mark><br>\n",
    "A limitation of probabilistic model selection methods is that the same general statistic cannot be calculated across a range of different types of models. Instead, the metric must be carefully derived for each model.<br>**\n",
    "It should be noted that the AIC statistic is designed for pre-planned comparisons between models (as opposed to comparisons of many models during automated searches).<br>\n",
    "A further limitation of these selection methods is that they do not take the uncertainty of the model into account.<br>\n",
    "**Such criteria do not take account of the uncertainty in the model parameters, however, and in practice they tend to favour overly simple models.** <br>\n",
    "\n",
    "There are three statistical approaches to estimating how well a given model fits a dataset and how complex the model is. And each can be shown to be equivalent or proportional to each other, although each was derived from a different framing or field of study.<br>\n",
    "They are:<br>\n",
    "\n",
    "- Akaike Information Criterion (AIC). Derived from frequentist probability.<br>\n",
    "- Bayesian Information Criterion (BIC). Derived from Bayesian probability.<br>\n",
    "- Minimum Description Length (MDL). Derived from information theory.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akaike Information Criterion\n",
    "The Akaike Information Criterion, or AIC for short, is a method for scoring and selecting a model.<br>\n",
    "It is named for the developer of the method, Hirotugu Akaike, and may be shown to have a basis in information theory and frequentist-based inference.<br>\n",
    "This is derived from a frequentist framework, and cannot be interpreted as an approximation to the marginal likelihood.\n",
    "\n",
    "\n",
    "The AIC statistic is defined for logistic regression as follows (taken from “The Elements of Statistical Learning“):\n",
    "$$ AIC = -2/N * LL + 2 * k/N $$\n",
    "Where N is the number of examples in the training dataset, LL is the log-likelihood of the model on the training dataset, and k is the number of parameters in the model.\n",
    "The score, as defined above, is minimized, e.g. the model with the lowest AIC is selected.\n",
    "\n",
    "<mark>To use AIC for model selection, we simply choose the model giving smallest AIC over the set of models considered.</mark>\n",
    "\n",
    "<mark>Compared to the BIC method (below), the AIC statistic penalizes complex models less, meaning that it may put more emphasis on model performance on the training dataset, and, in turn, select more complex models.</mark><br>\n",
    "We see that the penalty for AIC is less than for BIC. This causes AIC to pick more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Information Criterion\n",
    "The Bayesian Information Criterion, or BIC for short, is a method for scoring and selecting a model.\n",
    "It is named for the field of study from which it was derived: Bayesian probability and inference. Like AIC, it is appropriate for models fit under the maximum likelihood estimation framework.\n",
    "\n",
    "The BIC statistic is calculated for logistic regression as follows (taken from “The Elements of Statistical Learning“):\n",
    "$$ BIC = -2 * LL + log(N) * k $$\n",
    "Where log() has the base-e called the natural logarithm, LL is the log-likelihood of the model, N is the number of examples in the training dataset, and k is the number of parameters in the model.\n",
    "\n",
    "**The score as defined above is minimized, e.g. the model with the lowest BIC is selected.**\n",
    "<mark>The quantity calculated is different from AIC, although can be shown to be proportional to the AIC. Unlike the AIC, the BIC penalizes the model more for its complexity, meaning that more complex models will have a worse (larger) score and will, in turn, be less likely to be selected.</mark>\n",
    "Note that, compared to AIC […], this penalizes model complexity more heavily.\n",
    "\n",
    "Importantly, the derivation of BIC under the Bayesian probability framework means that if a selection of candidate models includes a true model for the dataset, then the probability that BIC will select the true model increases with the size of the training dataset. This cannot be said for the AIC score.\n",
    "… given a family of models, including the true model, the probability that BIC will select the correct model approaches one as the sample size N -> infinity.\n",
    "\n",
    "<mark>A downside of BIC is that for smaller, less representative training datasets, it is more likely to choose models that are too simple.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Description Length\n",
    "The Minimum Description Length, or MDL for short, is a method for scoring and selecting a model.<br>\n",
    "It is named for the field of study from which it was derived, namely information theory.<br>\n",
    "Information theory is concerned with the representation and transmission of information on a noisy channel, and as such, measures quantities like entropy, which is the average number of bits required to represent an event from a random variable or probability distribution.<br>\n",
    "<mark>From an information theory perspective, we may want to transmit both the predictions (or more precisely, their probability distributions) and the model used to generate them. Both the predicted target variable and the model can be described in terms of the number of bits required to transmit them on a noisy channel.<br>\n",
    "The Minimum Description Length is the minimum number of bits, or the minimum of the sum of the number of bits required to represent the data and the model.<br>\n",
    "The Minimum Description Length (MDL) principle recommends choosing the hypothesis that minimizes the sum of these two description lengths.</mark>\n",
    "\n",
    "The MDL statistic is calculated as follows (taken from “Machine Learning“):\n",
    "$$ MDL = L(h) + L(D | h) $$\n",
    "Where h is the model, D is the predictions made by the model, L(h) is the number of bits required to represent the model, and L(D | h) is the number of bits required to represent the predictions from the model on the training dataset.\n",
    "The score as defined above is minimized, e.g. the model with the lowest MDL is selected.\n",
    "The number of bits required to encode (D | h) and the number of bits required to encode (h) can be calculated as the negative log-likelihood; for example (taken from “The Elements of Statistical Learning“):\n",
    "$$ MDL = -log(P(theta)) – log(P(y | X, theta)) $$\n",
    "Or the negative log-likelihood of the model parameters (theta) and the negative log-likelihood of the target values (y) given the input values (X) and the model parameters (theta).<br>\n",
    "This desire to minimize the encoding of the model and its predictions is related to the notion of Occam’s Razor that seeks the simplest (least complex) explanation: in this context, the least complex model that predicts the target variable.<br>\n",
    "The MDL principle takes the stance that the best theory for a body of data is one that minimizes the size of the theory plus the amount of information necessary to specify the exceptions relative to the theory …\n",
    "\n",
    "The MDL calculation is very similar to BIC and can be shown to be equivalent in some situations.<br>\n",
    "Hence the BIC criterion, derived as approximation to log-posterior probability, can also be viewed as a device for (approximate) model choice by minimum description length.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
